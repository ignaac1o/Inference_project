---
title: "Inference_project"
author: Ignacio Almodóar & Limingrui Wan
output: html_document
---


\newpage

# 1. Introduction 

## 1.1 Description of the project and motivation. 

The aim of this project is to garner knowledge in the vehicles market, specifically about electric vehicles, which is a segment that is growing year by year.

The electric vehicle market has grown a lot in the last years and each year different brands are releasing new models with different specs and capacities. The automobile industry has been around us for almost 130 years, which means that almost everybody has at least a little bit of knowledge about the market in terms of traditional cars (combustion engines), if not probably people around you (relatives or friends) might know enough to give you recommendations that suits your needs.

However, what about electric cars? Most people struggle even telling prices for models that have already been in the market for more than five years, like Model S from Tesla or BMW i3, which are probably the ones that most people have seen several times on the streets, specially if you live in the city center.

As electric cars has always have that label of "expensive cars" or "useless noways", people still do not have the necessity to start considering them as feasible options to buy when looking for a new car. Which ends up with an uninformed society when it comes to this "new age" of cars.

Nevertheless this mentality has been changing, specially in the last years. It is not difficult to see several electric cars in one day, which means that people are starting to care about electric cars and to learn about them. However, it is sometimes difficult to compare between electric cars and traditional ones, because it is not part of what people is used to, and the prices might vary a lot for different models that might look the same just looking at the specs. Also, as new brands have come, many people do not know if their products are worth the price or not.

With this said, we have considered that this study could be useful for both companies and consumers. On one hand, companies that want to start developing electric cars might find helpful this study in order to consider several aspects while designing an electric car in terms of prices and characteristics. On the other hand for those consumers who want to take a peek in the electric cars market, allowing them to compare different prices in terms of power, efficiency or even number of seats and to see where those prices are going while the markets grows.

If you want to take the research on electric vehicles to the next level, the information that is given in this study might be very useful in order to compare between the traditional car market and the electric one. Also, you could make predictions about prices for the next years and which aspects are significant evolving.

## 1.2 Description of the data set

This data set has been taken from [kaggle](https://www.kaggle.com), which is a crowd-sourced platform to attract data scientists from all around the world to solve data science, machine learning and predictive analytics problems. It has more than one million of members, whereas more than half of the community are active members.

Kaggle enables data scientists and other developers to engage in running machine learning contests, write and share code, and to host data sets.

Even though we have taken the set from Kaggle, all the data that contains this data set is scrapped from an online electric cars gauge web [Electric Vehicle Database](https://ev-database.org/#sort:path~type~order=.rank~number~desc|range-slider-range:prev~next=0~1200|range-slider-acceleration:prev~next=2~23|range-slider-topspeed:prev~next=110~450|range-slider-battery:prev~next=10~200|range-slider-eff:prev~next=100~300|range-slider-fastcharge:prev~next=0~1500|paging:currentPage=0|paging:number=9), where you have different sections for finding and compare different electric vehicles.

This data set originally contains 177 different car models with 11 different aspects for each of them. These aspects are considered the most relevant ones while searching for information of different electric cars. These are also the ones that are directly related to the price and the ones that can be easily compared to one or another.   

## 1.3 Population of our sample size 

While searching through the web from where this data set has been scrapped, you can see that they have different sections for electric cars:

- Most recent
- Cheapest EV
- Most Efficient
- Quickest 0-100
- Longest range

In our case we have taken the ones listed for the cheapest electric car vehicles section, which necessary do not have only cheap cars, however it might show the most affordable ones from the ones with similar characteristics.

Notice that this data set is continuously updating, which means that you might get different results depending on when you do your research.

In short, we can consider that our population is the whole electric brand new market, whereas our sample is a summary of those who are considered the cheapest one for their characteristics.

## 1.4 Description of the variables

As we have mentioned, this data set contains eleven columns which basically tells us the most important aspects for every car.

On each of these different columns we will find different types of variables, some of them might be continuous, some others might be discrete with very few possible values, where ass others are discrete with a higher range of values.

On the other hand we have some of them that does not gives us much information or are difficult to understand if you are not very much into electric cars.

In essence, these are the variables that our data set contains:

- Name: This variable contains each car model name, including the brand and the exact model. This column strictly consists on string values, so we are not going to apply inference directly on it.

- Subtitle: Indicates the type of car and the capacity of the battery. The capacity of the batteries is measured in kWh, which is a unit of energy equal to one kilowatt of power sustained for one hour.

- Acceleration: Measures the time (in seconds) necessary for the model to accelerate from 0-100km/h. This is always a good indicator about the power that the engine provides in relation to the car weight. It is a continuous variable with range between 2.1 and 22.4 seconds.

- TopSpeed: Shows the maximum speed that the vehicle can reach. It is measured in Km/h and it shows how well does the car leverages the potential of its batteries. It is also a continuous variable with values between 123 and 410 km/h.

- Range: Shows the approximate distance a vehicle can travel with a 100% charge, which is always a good indicator about the real capacity of the batteries that the vehicle has in terms of common-daily use. It is measured in km and it is a continuous variable with range 95-970 km.

- Efficiency: This is a relatively difficult variable to understand. It calculates the battery energy consumption used by the vehicle for propulsion and on-board systems, which basically tells us the average consumption of energy per kilometers. This could be easily compared as the average liters per kilometer consumption of fuel for a combustion engine powered car. It is a continuous variable measured in Wh/km. Its range is 104-281 Wh/km

- FastChargeSpeed:This is also a strange variable because it is measured in km/h and it rates the average charging speed over a session from 10% to 80%. It is a continuous variable that fluctuates between 0 and 980 km/h for 0 meaning that it does not have fast charge mode.

- Drive: It defines which pair of wheels are the driving wheels of the vehicle, which is a very important attribute while buying a car. It is a discrete variable with three values (front wheel, Rear wheel and all wheel)

- NumberOfSeats: Indicates the number of legally seats available in the car. It is a discrete variable that oscillates between 2 and 7.

- PriceinGermany: It shows the retail price un Germany for a brand new model. It is a continuous variable and it is measured in Euros. The prices vary between 18460 and 215000 €.

- PriceinUKs: Same concept as before whereas in this case it for UK and it is measured in pounds. Notice that it is not strictly the conversion between euros and pounds, for most of them, the price will be higher in the UK.

This is a resume of all the variables that our data set contains. However we might not use some of them in this project as the information that they provide might not be very useful for the conclusions that we want to reach. Due to this, we might filter our data set in order to use only the columns that we find interesting.

\newpage

# 2. Model Selection

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
data=read.csv("EV_DATASET_woNA.csv",dec = ",")
data$PriceinGermany[is.na(data$PriceinGermany)]=data$PriceinUK[is.na(data$PriceinGermany)]*1.15
data=data[-c(29,120,129),]
data$PriceinGermany=data$PriceinGermany*1000
```


## 2.1 Probability distribuction of our continuous random variable "Price in Germany"

In order to get a quick view of how our variable is distributed, we have to plot it. A good way to view it is by plotting a histogram of the variable.

```{r echo=FALSE}
EURprice=data$PriceinGermany
```

```{r echo=FALSE}
hist(EURprice)
```

By analyzing this plot we can assume that it follows a gamma distribution $\Gamma(\alpha,\lambda)$. We are also going to transform our data applying logarithms to it and see if the distribution changes or not.

```{r echo=FALSE}
pricelog=log(EURprice)
hist(pricelog,prob=TRUE)
```

Even though when applying logarithm we get a much symmetric distribution,  it still looks more like a gamma distribution than a normal one so we can assume that our variable "PriceforGermany" follows a Poisson distribution.

## 2.2 Estimation of model parameters.

As we have concluded in the last point, we can assume that our random variable follows a gamma distribution $\Gamma(\alpha,\lambda)$ with parameters:

- Shape $k$, $\alpha=k$
- Scale $\theta$, $\lambda=\frac{1}{\theta}$ 

As we want to estimate our model parameters $\hat{\alpha}$ and $\hat{\lambda}$, we are going to use the method of moments in order estimate it.

The method of moments consists in leveling population moments and sample moments in order to get an equation or system of equations from where to get our wanted parameters.

For a gamma distribution, we get the following equations:

- $E[x]=\frac{1}{n}\sum_{i=0}^n x=\overline X$ for the fist population moment
- $E[x^2]=\frac{1}{n}\sum_{i=0}^n x_{^2}$ for the second population moment 

As we know that $E[x]=\frac{\alpha}{\lambda}$ for a Gamma distribution, from the first function we get that:

$$\alpha=\overline X\lambda$$

Replacing $\alpha$ on the second equation and simplifying it, we obtain:

$$\hat\lambda= \frac{\overline X}{\frac{1}{n}\sum_{i=0}^n x_{i}^2 - \overline{X}^2}$$

Whereas, using $\alpha=\overline X\lambda$, we get:

$$\hat\alpha=\overline X\hat\lambda=\frac{\overline X^2}{\frac{1}{n}\sum_{i=0}^n x_{i}^2 - \overline{X}^2}$$

Using these results we can calculate our estimators using R. We are going to compute the estimators of our both of our variables, with and without logarithm. This way we can then see which one of them has less error, therefore we will know which one is a better estimator. Running the code above we can say that our variable "Price in Germany" follows a $\Gamma(\hat\alpha,\hat\lambda)$ with parameters $\hat\alpha=3.3$ and $\hat\lambda=5.649*10^-5$ whereas if we take logarithm its parameters are $\hat\alpha=569.5397$ and $\hat\lambda=52.419$

```{r}
a=0
for(i in 1:length(EURprice)){
  a=a+EURprice[i]^2
}
hatlambda=(mean(EURprice))/((a/length(EURprice))-mean(EURprice)^2)
hatalpha=hatlambda*mean(EURprice)
hist(EURprice,prob=TRUE,breaks = 20)
r <- seq(min(EURprice), max(EURprice), length = 177)
f <- dgamma(r, hatalpha, hatlambda)
lines(r, f, col = "red", lwd = 2)
```

```{r echo=FALSE}
b=0
for(i in 1:length(pricelog)){
  b=b+pricelog[i]^2
}
hatlambdalog=(mean(pricelog))/((b/length(pricelog))-mean(pricelog)^2)
hatalphalog=hatlambdalog*mean(pricelog)
x=1:length(pricelog)
hist(pricelog,prob=TRUE,breaks = 20)
rlog =  seq(min(pricelog), max(pricelog), length = 177)
l <- dgamma(rlog, hatalphalog, hatlambdalog)
lines(rlog, l, col = "red", lwd = 2)
```

# 3. One-sample Inference

## 3.1 Estimators for our population mean

### unbiasedness

In this section we can use sample mean and sample median value to estimate our population mean.
```{r echo=FALSE}
samplemean=mean(pricelog)
sampleva=var(pricelog)
paste("the sample mean is ",samplemean)
paste("the sample median is ", median(pricelog))
```
we can quickly give the distribution of the sample mean:
$$\hat\theta \sim N(\mu,\sigma^2/n)$$
since we assumed a gamma distribution, we have $\mu=\frac{\alpha}{\beta}$ and $\sigma^2=\frac{\alpha}{\beta^2}$

obviously the sample mean is an unbiased estimator, as for the sample median value we know that gamma distribution is not symmetric. To proof this we can check again our plot.

```{r echo=FALSE}
hist(pricelog,prob=TRUE)
```

More specifically, we can get the pdf of the median value via the formula for order statistics, which is
$$\rho_k(x) = \frac{n!}{(k-1)!(n-k)!}(F(x))^{k-1}(1-F(x))^{n-k}\rho(x) $$
where n is the total number, k is the order number, $\rho(x)$ is the pdf of the general distribution, $F(x)$ is the cdf.

To proof if this median value we can compute the expectation of the estimator, so we shall calculate such indefinite integral:
$$\int x\rho_k(x) \space dx$$


## 3.2 Estimation of the errors of the two estimators
Since sample mean is an unbiased estimator, so we can calculate the error by CV, which is 
$$cv=\frac{sd(\hat \theta)}{exp(\hat \theta)} = \frac{\sqrt{\frac{\hat\alpha}{n\hat\beta^2}}}{\frac{\hat\alpha}{\hat\beta}} =\sqrt{\frac{1}{n\hat\alpha}}=0.017$$

as for the median value, we can compute the RRMSE $RRMSE=\sqrt{E(\hat\theta-\theta)^2}$

## 3.3 Confidence interval of 95% for our population mean
we are not having the true distribution of the price in population, thus we don't know the true mean-value and the  variance. Besides, we are not analyzing a normal distribution. So we can't compute the exact confidence intervals. As an alternative we use central limit theorem.

Firstly for any $X_1...X_n$ $i.i.d.$ with expectation $\mu$ and variance $V(x)=\sigma^2<\infty$,We have 

$$Z_n=\frac{\overline X-\mu}{S'/\sqrt{n}} \stackrel{d}\longrightarrow N(0,1)$$

because we can reform $Z_n$ as 
$$Z_n=\frac{\overline X-\mu}{S'/\sqrt{n}}=\frac{\overline X-\mu}{\sigma/\sqrt{n}}/\frac{S'}{\sigma}$$
where 
$$\frac{\overline X-\mu}{\sigma/\sqrt{n}}\stackrel{d}\longrightarrow N(0,1)$$
$$\frac{S'}{\sigma}\stackrel{p}\longrightarrow1$$
according to Slutsky's Theorem, we have
$$Z_n=\frac{\overline x-\mu}{S'/\sqrt{n}} \stackrel{d}\longrightarrow N(0,1)$$
so we need to find such $c_1,c_2 \space s.t.$

$$P(c_1<=u<=c_2)=1-\alpha$$
$$P(\frac{\overline x-c_1}{S'/\sqrt{n}}<=Z_n<=\frac{\overline x-c_2}{S'/\sqrt{n}})=1-\alpha$$
$$c_1>=\overline x -Z_{\alpha/2}\frac{S'}{\sqrt{n}} \space and \space c_2<= \overline x +Z_{\alpha/2}\frac{S'}{\sqrt{n}}$$
$$ CI_{1-\alpha}(\mu)=\overline x_n \pm Z_{\alpha/2} \frac{S'}{\sqrt n}$$
```{r echo=FALSE}
CI=c(samplemean+qnorm(0.025)*sqrt(sampleva),samplemean-qnorm(0.025)*sqrt(sampleva))
CI
```


## 3.4 Estimation of the proportion of units in the population that belongs to the category selected
We divide the sample into three groups according to the variable "drive", which implies how the specific car is driven. We consider a Binomial distribution$Bern(p)$, the estimator actually estimate the mean value of the probability in Binomial test, so the distribution of estimated proportion is$N(p,\frac{1(1-p)}{n})$
```{r echo=FALSE}
data$Drive=as.factor(data$Drive)
table(data$Drive)
plot(data$Drive)
```
the estimated proportion is:
```{r echo=FALSE}
p1=64/177
p2=68/177
p3=45/177
paste("the estimated proportion of All wheel Drive car is ",p1 )
paste("the estimated proportion of Front Wheel Drive car is ",p2 )
paste("the estimated proportion of Rear Wheel Drive  car is ",p3 )
```

## 3.5 Variance of our estimator of proportion
since we have obtained the distribution of $\overline x$,we can easily get the variance of the estimator:
```{r echo=FALSE}
n=177
v1=p1*(1-p1)/n
v2=p2*(1-p2)/n
v3=p3*(1-p3)/n
v1
v2
v3
```
## 3.6 Population proportion with 95% of confidence interval

from our previous work, we notice that the sample proportion actually is $\overline x$, and its distribution is 
$$\overline x \sim N(p,\frac{p(1-p)}{n})$$

so we have such conclusions:
$$\frac{\overline x-p}{\sqrt{\overline x(1-\overline x)}}\stackrel{d}\longrightarrow N(0,1)$$
$$\sqrt{\frac{\overline x(1-\overline x)}{p(1-p)}}\stackrel{p}\longrightarrow 1$$
so we can write a pivot 
$$Z=\frac{\overline x-p}{\sqrt{\frac{\overline x(1-\overline x)}{n}}}\stackrel{d}\longrightarrow N(0,1)$$
now we need to find such $c_1,c_2 \space s.t.$
$$P(c_1<=p<=c2)=1-\alpha$$
$$P(\frac{\overline x-c_1}{\sqrt{\frac{\overline x(1-\overline x)}{n}}}<=Z<=\frac{\overline x-c_2}{\sqrt{\frac{\overline x(1-\overline x)}{n}}})$$
notice in this case $\hat p=\overline x$, we have such conclusions

$$\hat p \pm Z_{\alpha/2} \sqrt{\frac{\hat p(1-\hat p)}{n}}$$
```{r echo=FALSE}
c(p1+qnorm(0.025)*sqrt(v1),p1-qnorm(0.025)*sqrt(v1))
c(p2+qnorm(0.025)*sqrt(v2),p2-qnorm(0.025)*sqrt(v2))
c(p3+qnorm(0.025)*sqrt(v3),p3-qnorm(0.025)*sqrt(v3))
```

\newpage


# 4 Inference with more than one sample

```{r echo=FALSE}
data$size=1
data$size[data$NumberofSeats!=7]=0
table(data$NumberofSeats)
data_smallcar=data[data$size==0,]
data_bigcar=data[data$size==1,]
```

## 4.1 population mean of each group

In this section, we choose to divide the sample and population into two subgroups according to the variable *numberofseats*. This variable has four possible values :2,4,5,7. But we can notice that some subgroups have only a few items, so we decide to create a new factor variable *size*. We defined the cars with 5 or less seats as *small* cars, marked as *0*, and the cars with 7 seats as *big* cars, marked as *1*.
```{r}
barplot(table(data$size))
```
as we can see that we have 145 small cars and 32 big cars in the sample, and we assume for each group the distribution is still gamma. 
In general, the case could be describe mathematically as: There is a s.r.s. $(x_{11},x_{12} ,\dots,x_{1n_1})$ from $X_1 \sim Ga(\alpha_1,\beta_1)$ , and another s.r.s. $(x_{21},x_{22},\dots,x_{2n_2})$ from $X_2 \sim Ga(\alpha_2,\beta_2)$, where $n_1=145, n_2=32$,$\alpha_1,\alpha_2,\beta_1,\beta_2$ unknown. $X_1,X_2$ independent. 

We can estimate the population mean of *price* in our data, as we did in section 3.1, using sample mean as estimator.

$$\hat\mu_1=\overline x_1, \space\hat\mu_2=\overline x_2 $$

```{r echo=FALSE}
n1=length(data_smallcar$PriceinGermany)
n2=length(data_bigcar$PriceinGermany)
samplemean_s=mean(data_smallcar$PriceinGermany)
samplemean_b=mean(data_bigcar$PriceinGermany)
paste("mean price of the small cars is", samplemean_s)
paste("mean price of the big cars is", samplemean_b)
```
in order to get the coefficient of variation, we need to know the variance of the estimator, we can use $S'^2$ to estimate the population variance.
```{r echo=FALSE}
var_1=0
for (i in 1:n1){
  var_1=var_1+((samplemean_s-data_smallcar$PriceinGermany[i])^2)
}
var_1=var_1/(n1-1)

var_2=0
for (i in 1:n2){
  var_2=var_2+((samplemean_b-data_bigcar$PriceinGermany[i])^2)
}
var_2=var_2/(n1-1)
paste("estimated variance of price in small cars is ", var_1)
paste("estimated variance of price in big cars is", var_2)
```
so far we give the distribution of our estimator and compute their CVs:
$$\hat\mu_1 \sim N(\overline x_1,v(x_1)/n_1),\hat\mu_2 \sim N(\overline x_2,v(x_2)/n_2)$$
```{r echo=FALSE}
paste("cv of the small cars' prices is", ((var_1/n1)^0.5)/samplemean_s)
paste("cv of the big cars' prices is", ((var_2/n2)^0.5)/samplemean_b)
```

# additional analyze: using bootstrap to interpret the estimator

In section 3, we interpret the properties of two estimators *sample mean*,*sample median value* of the population mean. Considering our sample size is large enough, we can use *CLT* to obtain the approximated distribution of *sample mean*, thus is not very hard to obtain it's properties including *unbiasedness*,*efficiency* even more.

However, as shown in section 3, is always very hard to get the exact distribution of *sample median value* ,especially without normal distribution. For this reason, we only gave the expectation and variance of the *sample median value* from some references. 

To do inference for estimator or the parameters with a more complex distribution, we can now introduce *bootstrap*. The *bootstrap* method mainly using the idea of *resampling*. Here is a brief introduction of this method.

Assuming we have a $s.r.s. \space (x_1,x_2,\dots,x_1000)$ from a unknown distribution $X\sim F(.)$, similarly, we want to estimate the population mean of X by sample mean.
we first randomly choose 500 items form the whole sample, and compute the mean value and record it as *m_1*, then we repeat this step for 1000 times. Here we are having a set of sample means$(m_1,m_2,\dots,m_{500})$, and with this new set, we can give the expectation and variance of our estimator,
$$exp(samplemean)= \overline m, var(sample mean)=var(m)$$
further more, we can reorganize the set $(m_1,m_2,\dots,m_{500})$ from the least to the most, and so we can determine the upper-bound and the lower-bound with 95% quantiles (actually 2.5% and 97.5%) of the 95% confidence interval. that is:

$$CI(sample mean)=(m_{(0.025*500)},m_{(0.975*500)})$$
Meanwhile, if we want to estimate the sample median value, all we need to do is to compute the median value of each sub-sample.

In R, we can use the package $boostrap$, or as we are not doing a complex computation ,we can do it manually. 

```{r}
n_iteration=1000 # number of repetition
i_resample=100 # size of each subsample
vector_mean=vector()# the set of mean values
vector_median=vector()# the set of median values

for(i in 1:n_iteration){
  m=sample(EURprice,size=i_resample,replace = FALSE)
  vector_mean[i]=mean(m)
  vector_median[i]=median(m)
}
mean_exp=mean(vector_mean)
median_exp=mean(vector_median)

mean_var=var(vector_mean)
median_var=var(vector_median)
```

we can visualize these two vectors:

```{r echo=FALSE}
mfrow=c(1,2)
hist(vector_mean)
hist(vector_median)
```

this way we obtained two estimator of population mean by $bootstrap$ methods. And we can give their expectations and variances
```{r echo=FALSE}
paste("the expectation of sample mean is:", mean_exp)
paste("the expectation of sample median is:", median_exp)
paste("the variance of the sample mean is:",mean_var)
paste("the variance of the sample median is:",median_var)
```

To obtain their confidence intervals, we need to reorganize the vectors and get the quantiles

```{r}
mean_ordered=sort(vector_mean)
median_ordered=sort(vector_median)

upper_mean=mean_ordered[975]
lower_mean=mean_ordered[25]
## confidence interval bounds of sample mean
upper_med=median_ordered[975]
lower_med=median_ordered[25]
## confidence interval bounds of sample median
```

thus we have the estimators' 95% confidence intervals:
```{r echo=FALSE}
print("the 95% CI of sample mean is: ")
c(lower_mean,upper_mean)

print("the 95% CI of sample median is:")
c(lower_med,upper_med)
```

we can see that there is an obvious difference between the expectations, and the variance of sample means is larger, that is to say, more scattered.

