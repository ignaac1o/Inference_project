---
title: "Inference_project"
author: Ignacio Almodóar & Limingrui Wan
output: html_document
---


\newpage

# 1. Introduction 

## 1.1 Description of the project and motivation. 

The aim of this project is to garner knowledge in the vehicles market, specifically about electric vehicles, which is a segment that is growing year by year.

The electric vehicle market has grown a lot in the last years and each year different brands are releasing new models with different specs and capacities. The automobile industry has been around us for almost 130 years, which means that almost everybody has at least a little bit of knowledge about the market in terms of traditional cars (combustion engines), if not probably people around you (relatives or friends) might know enough to give you recommendations that suits your needs.

However, what about electric cars? Most people struggle even telling prices for models that have already been in the market for more than five years, like Model S from Tesla or BMW i3, which are probably the ones that most people have seen several times on the streets, specially if you live in the city center.

As electric cars has always have that label of "expensive cars" or "useless noways", people still do not have the necessity to start considering them as feasible options to buy when looking for a new car. Which ends up with an uninformed society when it comes to this "new age" of cars.

Nevertheless this mentality has been changing, specially in the last years. It is not difficult to see several electric cars in one day, which means that people are starting to care about electric cars and to learn about them. However, it is sometimes difficult to compare between electric cars and traditional ones, because it is not part of what people is used to, and the prices might vary a lot for different models that might look the same just looking at the specs. Also, as new brands have come, many people do not know if their products are worth the price or not.

With this said, we have considered that this study could be useful for both companies and consumers. On one hand, companies that want to start developing electric cars might find helpful this study in order to consider several aspects while designing an electric car in terms of prices and characteristics. On the other hand for those consumers who want to take a peek in the electric cars market, allowing them to compare different prices in terms of power, efficiency or even number of seats and to see where those prices are going while the markets grows.

If you want to take the research on electric vehicles to the next level, the information that is given in this study might be very useful in order to compare between the traditional car market and the electric one. Also, you could make predictions about prices for the next years and which aspects are significant evolving.

## 1.2 Description of the data set

This data set has been taken from [kaggle](https://www.kaggle.com), which is a crowd-sourced platform to attract data scientists from all around the world to solve data science, machine learning and predictive analytics problems. It has more than one million of members, whereas more than half of the community are active members.

Kaggle enables data scientists and other developers to engage in running machine learning contests, write and share code, and to host data sets.

Even though we have taken the set from Kaggle, all the data that contains this data set is scrapped from an online electric cars gauge web [Electric Vehicle Database](https://ev-database.org/#sort:path~type~order=.rank~number~desc|range-slider-range:prev~next=0~1200|range-slider-acceleration:prev~next=2~23|range-slider-topspeed:prev~next=110~450|range-slider-battery:prev~next=10~200|range-slider-eff:prev~next=100~300|range-slider-fastcharge:prev~next=0~1500|paging:currentPage=0|paging:number=9), where you have different sections for finding and compare different electric vehicles.

This data set originally contains 177 different car models with 11 different aspects for each of them. These aspects are considered the most relevant ones while searching for information of different electric cars. These are also the ones that are directly related to the price and the ones that can be easily compared to one or another.   

## 1.3 Population of our sample size 

While searching through the web from where this data set has been scrapped, you can see that they have different sections for electric cars:

- Most recent
- Cheapest EV
- Most Efficient
- Quickest 0-100
- Longest range

In our case we have taken the ones listed for the cheapest electric car vehicles section, which necessary do not have only cheap cars, however it might show the most affordable ones from the ones with similar characteristics.

Notice that this data set is continuously updating, which means that you might get different results depending on when you do your research.

In short, we can consider that our population is the whole electric brand new market, whereas our sample is a summary of those who are considered the cheapest one for their characteristics.

## 1.4 Description of the variables

As we have mentioned, this data set contains eleven columns which basically tells us the most important aspects for every car.

On each of these different columns we will find different types of variables, some of them might be continuous, some others might be discrete with very few possible values, where ass others are discrete with a higher range of values.

On the other hand we have some of them that does not gives us much information or are difficult to understand if you are not very much into electric cars.

In essence, these are the variables that our data set contains:

- Name: This variable contains each car model name, including the brand and the exact model. This column strictly consists on string values, so we are not going to apply inference directly on it.

- Subtitle: Indicates the type of car and the capacity of the battery. The capacity of the batteries is measured in kWh, which is a unit of energy equal to one kilowatt of power sustained for one hour.

- Acceleration: Measures the time (in seconds) necessary for the model to accelerate from 0-100km/h. This is always a good indicator about the power that the engine provides in relation to the car weight. It is a continuous variable with range between 2.1 and 22.4 seconds.

- TopSpeed: Shows the maximum speed that the vehicle can reach. It is measured in Km/h and it shows how well does the car leverages the potential of its batteries. It is also a continuous variable with values between 123 and 410 km/h.

- Range: Shows the approximate distance a vehicle can travel with a 100% charge, which is always a good indicator about the real capacity of the batteries that the vehicle has in terms of common-daily use. It is measured in km and it is a continuous variable with range 95-970 km.

- Efficiency: This is a relatively difficult variable to understand. It calculates the battery energy consumption used by the vehicle for propulsion and on-board systems, which basically tells us the average consumption of energy per kilometers. This could be easily compared as the average liters per kilometer consumption of fuel for a combustion engine powered car. It is a continuous variable measured in Wh/km. Its range is 104-281 Wh/km

- FastChargeSpeed:This is also a strange variable because it is measured in km/h and it rates the average charging speed over a session from 10% to 80%. It is a continuous variable that fluctuates between 0 and 980 km/h for 0 meaning that it does not have fast charge mode.

- Drive: It defines which pair of wheels are the driving wheels of the vehicle, which is a very important attribute while buying a car. It is a discrete variable with three values (front wheel, Rear wheel and all wheel)

- NumberOfSeats: Indicates the number of legally seats available in the car. It is a discrete variable that oscillates between 2 and 7.

- PriceinGermany: It shows the retail price un Germany for a brand new model. It is a continuous variable and it is measured in Euros. The prices vary between 18460 and 215000 €.

- PriceinUKs: Same concept as before whereas in this case it for UK and it is measured in pounds. Notice that it is not strictly the conversion between euros and pounds, for most of them, the price will be higher in the UK.

This is a resume of all the variables that our data set contains. However we might not use some of them in this project as the information that they provide might not be very useful for the conclusions that we want to reach. Due to this, we might filter our data set in order to use only the columns that we find interesting.

\newpage

# 2. Model Selection

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
data=read.csv("EV_DATASET_woNA.csv",dec = ",")
data$PriceinGermany[is.na(data$PriceinGermany)]=data$PriceinUK[is.na(data$PriceinGermany)]*1.15
data=data[-c(29,120,129),]
data$PriceinGermany=data$PriceinGermany*1000
```


## 2.1 Probability distribuction of our continuous random variable "Price in Germany"

In order to get a quick view of how our variable is distributed, we have to plot it. A good way to view it is by plotting a histogram of the variable.

```{r echo=FALSE}
EURprice=data$PriceinGermany
```

```{r echo=FALSE}
hist(EURprice)
```

By analyzing this plot we can assume that it follows a gamma distribution $\Gamma(\alpha,\lambda)$. We are also going to transform our data applying logarithms to it and see if its shape can be estimated as a normal.

```{r echo=FALSE}
pricelog=log(EURprice)
hist(pricelog,prob=TRUE)
```

Applying logarithm we get a much symmetric distribution, that can be considered as a normal distribution.

Given this results we can consider that our variable "PriceforGermany" follows a gamma distribution, whereas applying logarithm transformation to it makes it look more to a normal distribution.

## 2.2 Estimation of model parameters.

In this point we are going to estimate our model parameters for our variable "PriceforGermany" with and without the logarithm transformation in order to know which one is better estimated

We have assumed that our random variable "PriceforGermany" follows a gamma distribution $\Gamma(\alpha,\lambda)$ with parameters:

- Shape $k$, $\alpha=k$
- Scale $\theta$, $\lambda=\frac{1}{\theta}$ 

As we want to estimate our model parameters $\hat{\alpha}$ and $\hat{\lambda}$, we are going to use the method of moments in order estimate it.

The method of moments consists in leveling population moments and sample moments in order to get an equation or system of equations from where to get our wanted parameters.

For a gamma distribution, we get the following equations:

- $E[x]=\frac{1}{n}\sum_{i=0}^n x=\overline X$ for the fist population moment
- $E[x^2]=\frac{1}{n}\sum_{i=0}^n x_{^2}$ for the second population moment 

As we know that $E[x]=\frac{\alpha}{\lambda}$ for a Gamma distribution, from the first function we get that:

$$\alpha=\overline X\lambda$$

Replacing $\alpha$ on the second equation and simplifying it, we obtain:

$$\hat\lambda= \frac{\overline X}{\frac{1}{n}\sum_{i=0}^n x_{i}^2 - \overline{X}^2}$$

Whereas, using $\alpha=\overline X\lambda$, we get:

$$\hat\alpha=\overline X\hat\lambda=\frac{\overline X^2}{\frac{1}{n}\sum_{i=0}^n x_{i}^2 - \overline{X}^2}$$

Using these results we can calculate our estimators using R. We are going to compute the estimators of our variable. Running the code above we can say that our variable "Price in Germany" follows a $\Gamma(\hat\alpha,\hat\lambda)$ with parameters $\hat\alpha=3.3$ and $\hat\lambda=5.649*10^-5$.

```{r}
a=0
for(i in 1:length(EURprice)){
  a=a+EURprice[i]^2
}
hatlambda=(mean(EURprice))/((a/length(EURprice))-mean(EURprice)^2)
hatalpha=hatlambda*mean(EURprice)
hist(EURprice,prob=TRUE,breaks = 20)
r <- seq(min(EURprice), max(EURprice), length = 177)
f <- dgamma(r, hatalpha, hatlambda)
lines(r, f, col = "red", lwd = 2)
```

For the logarithm transformation we are considering that our variable follows a normal distribution so, in order to estimate our parameters we are going yo use the Maximum Likelihood method. For this purpose we have to compute $\frac{\partial}{\partial\theta_{k}}L(\theta;x_{1},...,x_{n})=0$. This will give us a system of equations from where we can obtain each of our estimators.

Therefore, for a normal distribution we have:

$$L(\mu,\sigma^2,x_{1},...,x_{n})=\frac{1}{\sqrt(2\pi\sigma^2)}e^{(\frac{-1}{2\sigma^2}\sum_{i=1}^{n}(x_{i}-\mu)^2)}$$
Applying logarithm to the function we can simplify it.

$$l(\mu,\sigma^2,x_{1},...,x_{n})=-\frac{n}{2}log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(x_{i}-\mu)^2)$$
As we want to obtain the maximum of the loglikelihood, we get the following system of equations:

$$\frac{\partial l}{\partial\mu}=\frac{1}{\sigma^2}\sum_{i=1}^n(x_{i}-\mu)=0 $$
$$\frac{\partial l}{\partial\sigma^2}=\frac{-n}{2\sigma^2}+\frac{\sum_{i=1}^n(x_{i}-\mu)^2}{2\sigma^4}=0 $$
Solving the system we finally get our estimators:

$$\hat\mu=\overline{x}_n$$
$$\hat\sigma=\frac{1}{n}\sum(x_i-\overline{x})^2$$
By the results obtained we can see that our estimator for $\hat\mu$ is the sample mean, whereas for $\hat\sigma$ we have the sample variance.

```{r }
hatmu=mean(pricelog)
hatsigma=var(pricelog)
x=1:length(pricelog)
hist(pricelog,prob=TRUE,breaks = 20,ylim=c(0,1.5))
rlog =  seq(min(pricelog), max(pricelog), length = 177)
l <- dnorm(rlog, hatmu, sqrt(hatsigma))
lines(rlog, l, col = "red", lwd = 2)
```

Within this last plot we can see that the estimated distribution for our variable "PriceforGermany" transformed with logarithm is very close to the distribution of the variable so we can consider that it follows a normal distribution. 

# 3. One-sample Inference

## 3.1 Estimators for our population mean

### unbiasedness

In this section we can use sample mean and sample median value to estimate our population mean.
```{r echo=FALSE}
samplemean=mean(pricelog)
sampleva=var(pricelog)
paste("the sample mean is ",samplemean)
paste("the sample median is ", median(pricelog))
```
we can quickly give the distribution of the sample mean with CLT:
$$\hat\theta \sim N(\mu,\sigma^2/n)$$

obviously the sample mean is an unbiased estimator, since $exp(\hat\theta)=\mu$. Meanwhile, as we assume that it's a normal distribution, it's symmetric, thus sample median value is also unbiased $exp(med)=\mu$ and the variance of sample mean is$\hat \sigma^2/n=0.001$

However, it's not easy to obtain the real distribution of sample median value(to get the expectation and variance), firstly we get the pdf of the sample median value via the formula for order statistics, which is

$$\rho_k(x) = \frac{n!}{(k-1)!(n-k)!}(F(x))^{k-1}(1-F(x))^{n-k}\rho(x) $$
where n is the total number, k is the sequence number(2/n when we are ), $\rho(x)$ is the pdf of the original distribution(normal distribution), $F(x)$ is the cdf of the original distribution. So we shall calculate such indefinite integral:
$$\int x\rho_k(x) \space dx$$

so far we can see the computation is too complex, but the variance is given in the slides or others reference
$$V(med) \approx \frac{\pi \sigma^2}{2n}=0.016$$

In order to check if the estimator is efficient, we just have to check their fisher information$I_n(\theta)=\frac {n}{\sigma^2}=v(\overline x)<v(med)$, so only sample mean is efficient.

Obviously, sample mean has smaller variance and more efficient, but median value is more robust, can afford more missing of sample or more outliers.

## 3.2 Estimation of the errors of the two estimators
Since sample mean and sample median value are both unbiased estimators, so we can calculate the error by CV, which is 
$$cv=\frac{sd(\hat \theta)}{exp(\hat \theta)} $$

```{r}
cv1=(hatsigma)^0.5/hatmu
cv2=cv1*1.57
paste("the error(cv) of sample mean is",cv1)
paste("the error(cv) of sample median value is",cv2)

```


## 3.3 Confidence interval of 95% for our population mean

In order to obtain the upper bound and the lower bound, we need to find such $c_1,c_2$ s.t.
$$P(c_1<\mu<c_2)=1-\alpha$$
We are having a normal assumption of the distribution, so we use *exact confidence intervals under normality*, and of course we can use T-statistic. Describe our problem in a mathematical way:

We have $(x_1,x_2,\dots,x_n)$s.r.s. form a r.v.$N(\mu,\sigma^2)$, where $\mu, \sigma$unknown, and $n=177$, it holds
$$T=\frac{\overline x-\mu}{S_n'/\sqrt{n}}\sim t_{n-1}$$

then we just have to find such $t_1,t_2$ s.t.
$$P(t_1<T<t_2)=1-\alpha$$
$$P(\frac{\overline x-c_1}{S'/\sqrt n}<\frac{\overline x-\mu}{S'/\sqrt n}<\frac{\overline x-c_2}{S'/\sqrt n})=1-\alpha$$

$$P(\frac{\overline x-c_1}{S'/\sqrt n}<T<\frac{\overline x-c_2}{S'/\sqrt n})=1-\alpha$$
so we have 
$$c_1=\overline x +t_{n-1,\alpha/2}*\frac{S'}{\sqrt n},c_2=\overline x -t_{n-1,\alpha/2}*\frac{S'}{\sqrt n}$$
the CI is
$$CI_{1-\alpha}(\mu)=(\overline x +t_{n-1,\alpha/2}*\frac{S'}{\sqrt n},\overline x -t_{n-1,\alpha/2}*\frac{S'}{\sqrt n})$$
where $\alpha=0.05$

```{r echo=FALSE}
CI=c(samplemean+qt(0.025,df=176)*sqrt(sampleva),samplemean+qt(0.975,df=176)*sqrt(sampleva))
print("95% CI of population mean is:")
CI
```


## 3.4 Estimation of the proportion of units in the population that belongs to the category selected
We divide the sample into three groups according to the variable "drive", which implies how the specific car is driven. We consider a Binomial distribution$Bern(p)$, the estimator actually estimate the mean value of the probability in a Binomial test, so the distribution of estimated proportion is$N(p,\frac{1(1-p)}{n})$
```{r echo=FALSE}
data$Drive=as.factor(data$Drive)
table(data$Drive)
plot(data$Drive)
```
the estimated proportion is:
```{r echo=FALSE}
p1=64/177
p2=68/177
p3=45/177
paste("the estimated proportion of All wheel Drive car is ",p1 )
paste("the estimated proportion of Front Wheel Drive car is ",p2 )
paste("the estimated proportion of Rear Wheel Drive  car is ",p3 )
```

## 3.5 Variance of our estimator of proportion
since we have obtained the distribution of $\overline x$,we can easily get the variance of the estimator:
```{r echo=FALSE}
n=177
v1=p1*(1-p1)/n
v2=p2*(1-p2)/n
v3=p3*(1-p3)/n
v1
v2
v3
```
## 3.6 Population proportion with 95% of confidence interval

from our previous work, we notice that the sample proportion actually is $\overline x$, and its distribution is 
$$\overline x \sim N(p,\frac{p(1-p)}{n})$$

so we have such conclusions:
$$\frac{\overline x-p}{\sqrt{\overline x(1-\overline x)}}\stackrel{d}\longrightarrow N(0,1)$$
$$\sqrt{\frac{\overline x(1-\overline x)}{p(1-p)}}\stackrel{p}\longrightarrow 1$$
according to  Slutsky' th we know that
$$Z=\frac{\overline x-p}{\sqrt{\frac{\overline x(1-\overline x)}{n}}}\stackrel{d}\longrightarrow N(0,1)$$
so now we only need to find such $c_1,c_2 \space s.t.$
$$P(c_1<=p<=c2)=1-\alpha$$
$$P(\frac{\overline x-c_1}{\sqrt{\frac{\overline x(1-\overline x)}{n}}}<=Z<=\frac{\overline x-c_2}{\sqrt{\frac{\overline x(1-\overline x)}{n}}})=1-\alpha$$
notice in this case $\hat p=\overline x$, we have such bounds

$$\hat p \pm Z_{\alpha/2} \sqrt{\frac{\hat p(1-\hat p)}{n}}$$
```{r echo=FALSE}
c(p1+qnorm(0.025)*sqrt(v1),p1-qnorm(0.025)*sqrt(v1))
c(p2+qnorm(0.025)*sqrt(v2),p2-qnorm(0.025)*sqrt(v2))
c(p3+qnorm(0.025)*sqrt(v3),p3-qnorm(0.025)*sqrt(v3))
```

\newpage


# 4 Inference with more than one sample

```{r echo=FALSE}
data$size=1
data$size[data$NumberofSeats!=7]=0
table(data$NumberofSeats)
data_smallcar=data[data$size==0,]
data_bigcar=data[data$size==1,]
```

## 4.1 population mean of each group

In this section, we choose to divide the sample and population into two subgroups according to the variable *numberofseats*. This variable has four possible values :2,4,5,7. But we can notice that some subgroups have only a few items, so we decide to create a new factor variable *size*. We defined the cars with 5 or less seats as *small* cars, marked with *0*, and the cars with 7 seats as *big* cars, marked with *1*.

```{r}
barplot(table(data$size))
```

As we can see that we have 145 small cars and 32 big cars in the sample, and we assume for each group the distribution is still gamma. 

In general, the case could be describe mathematically as: There is a s.r.s. $(x_{11},x_{12} ,\dots,x_{1n_1})$ from $X_1 \sim Ga(\alpha_1,\beta_1)$ , and another s.r.s. $(x_{21},x_{22},\dots,x_{2n_2})$ from $X_2 \sim Ga(\alpha_2,\beta_2)$, where $n_1=145, n_2=32$,$\alpha_1,\alpha_2,\beta_1,\beta_2$ unknown. $X_1,X_2$ independent. 

We can estimate the population mean of *price* in our data, as we did in section 3.1, using sample mean as estimator.

$$\hat\mu_1=\overline x_1, \space\hat\mu_2=\overline x_2 $$

```{r echo=FALSE}
n1=length(data_smallcar$PriceinGermany)
n2=length(data_bigcar$PriceinGermany)
samplemean_s=mean(data_smallcar$PriceinGermany)
samplemean_b=mean(data_bigcar$PriceinGermany)
paste("mean price of the small cars is", samplemean_s)
paste("mean price of the big cars is", samplemean_b)
```
in order to get the coefficient of variation, we need to know the variance of the estimator, we can use $S'^2$ to estimate the population variance.
```{r echo=FALSE}
var_1=0
for (i in 1:n1){
  var_1=var_1+((samplemean_s-data_smallcar$PriceinGermany[i])^2)
}
var_1=var_1/(n1-1)

var_2=0
for (i in 1:n2){
  var_2=var_2+((samplemean_b-data_bigcar$PriceinGermany[i])^2)
}
var_2=var_2/(n1-1)
paste("estimated variance of price in small cars is ", var_1)
paste("estimated variance of price in big cars is", var_2)
```
so far we give the distribution of our estimator and compute their CVs:
$$\hat\mu_1 \sim N(\overline x_1,v(x_1)/n_1),\hat\mu_2 \sim N(\overline x_2,v(x_2)/n_2)$$
```{r echo=FALSE}
paste("cv of the small cars' prices is", ((var_1/n1)^0.5)/samplemean_s)
paste("cv of the big cars' prices is", ((var_2/n2)^0.5)/samplemean_b)
```

## 4.2 Estimate the proportion of Section 3.4 for each population considered in 4.1. Estimate MSE of your estimators.
Similarly, we also consider a binomial distribution and then apply CLT to get the estimated proportion.
```{r echo=FALSE}
print("the estimated porportions for different driven ways in big-car are: ")
table(data_bigcar$Drive)/n2
print("the estimated porportions for different driven ways in small-car are: ")
table(data_smallcar$Drive)/n1
```

Considering sample mean is an unbiased estimator for population mean, $MSE(\hat p)= B^2(\hat p)+V(\hat p)=0+V(\hat p)=V(\hat p)$. As shown in section 3.4 $V(\hat p)=\frac{\hat p(1-\hat p)}{n}$, where n is the sample size.
```{r}
print("MSEs in big-car group are:")
table(data_bigcar$Drive)/n2*(1-table(data_bigcar$Drive)/n2)/n2
print("MSEs in small-car group are:")
table(data_smallcar$Drive)/n1*(table(data_smallcar$Drive)/n1)/n1
```

## 4.3 Using the two subgroups considered in 4.1. Compare the means of the continuous variable for the two subgroups by means of a confidence interval for the difference of means.

We have now two random samples, one with all the cars that have 5 or less seats and another one formed by all the cars that have more than 7 seats. As these are random samples we do not know each population mean and variance.

In order to solve the problem we have to find a pivotal quantity where all the values in it are known. For this reason we are going to use de T statistics where mean and variances are estimated as sample means and sample cuasivariances.

$$T=\frac{\overline X_{1}-\overline{X}_2- (\mu_{1}-\mu_{2})}{\sqrt{\frac{S´_1^2}{n_1}+\frac{S´_2^2}{n_2}}} \sim t_v $$
This expression can be considered as a normal due to the Welch-Sattertwaite method, which is used to calculate an approximation to the effective degrees of freedom of a linear combination of independent sample variances. Using this method, we get thw following expression for the degrees of freedom:

$$v=\frac{(\frac{S´_1^2}{n_1}+\frac{S´_2^2}{n_2})^2}{\frac{S´_1^4}{n_1^2(n_1-1)}+\frac{S´_2^4}{n_2^2(n_2-1)}}$$
Now that we have our pivotal quantity we have to find our confidence intervals such that $P(c_1\leq T \leq c_2) = 1- \alpha$. Within a few changes our expression to calculate confidence intervals ends up being:

$$CI_{1-\alpha}(\mu_1-\mu_2)= \overline X_1 + \overline X_2 \pm t_{v,1-\alpha/2}\sqrt{\frac{S´_1^2}{n_1}+\frac{S´_2^2}{n_2}}$$
Then, a confidence interval for $\mu_1-\mu_2$ at a confidence level of 0.95 is:
```{r, echo = FALSE}
sm1=mean(log(data_smallcar$PriceinGermany))
sm2=mean(log(data_bigcar$PriceinGermany))
sv1=var(log(data_smallcar$PriceinGermany))
sv2=var(log(data_bigcar$PriceinGermany))
n1=length(data_smallcar$PriceinGermany)
n2=length(data_bigcar$PriceinGermany)

v=(((sv1/n1)+(sv2/n2))^2)/((sv1^2/(n1^2*(n1-1)))+(sv1^2/(n1^2*(n1-1))))
alpha=0.05
t=qt(alpha/2,df=v,lower.tail = FALSE)

```
```{r}
(sm1-sm2)+c(-1,1)*t*sqrt((sv1/n1)+(sv2/n2))
```
 So our $CI_{0.95}(\mu_1-\mu_2)=(-0.25,-0.0319)$
 
 [ref:theorema1](https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/PASS/Confidence_Intervals_for_the_Difference_Between_Two_Means.pdf)
 [ref:theorema2](https://en.wikipedia.org/wiki/Welch–Satterthwaite_equation)

## 4.4 Compare the means of the continuous variable for the two subgroups by means of hypothesis testing of equality of means.

We assume now the same two populations as in the section 4.3. We will test hypotheses about the difference of means $\mu_1-\mu_2$. We are going to test if there is any major difference in the mean for the prices of the cars in Germany depending on the number of seats at a 5% level of significance.

Denoting $\theta=\mu_1-\mu_2$ we want to test:

$H_{o}: \mu_1-\mu_2$   vs. $H_1: \mu_1 \neq \mu_2$

$H_{o}: \theta=0$ vs. $\theta: \mu_1 \neq \mu_2$

As $\sigma_1^2$ and $\sigma_2^2$ are unknowns, we define our critical region is $Cc=\{|T|>t_{v,1-\alpha/2}\}$ for $v$ denoting the degrees of freedom seen in section 4.3.

Therefore, the test statistic that we are going to use is:

$$T=\frac{\overline X_{1}-\overline{X}_2- (\mu_{1}-\mu_{2})}{\sqrt{\frac{S´_1^2}{n_1}+\frac{S´_2^2}{n_2}}}$$
```{r, echo=FALSE}
t_score=(sm1-sm2)/(sqrt((sv1/n1)+(sv2/n2)))
```

Solving both equations we get the following results, $T=-2.53$ and $t_{v,1-\alpha/2}=1.96$. Therefore we can see that our T value is is in the critical region. Within this result we need to reject the null hypothesis. Then we can conclude that there is a major significance difference at a 5% significance level that there is a difference between the mean of the prices for cars with less than 5 seats and more than 7.

## 4.5 Compare the proportion of Section 4.4 for the subgroups by means of a confidence interval for the difference of proportions.
first consider a general case, a s.r.s$(x_{11},x_{12},\dots,x_{1n_1})$ from $X_1\sim N(\mu_1,\sigma _1^2)$, another s.r.s$(x_{21},x_{22},\dots,x_{2n_2})$ from $X_2\sim N(\mu_2,\sigma _2^2)$, then we have $\theta =x_1+x_2 \sim(\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)$

In our case, from above sections we know that the proportions are independently following different normal distributions. We firstly assume a variable $\theta =p_1-p_2$, where $p_1$ represents the proportions from group *big-car*, $p_2$ represents the proportions from group *small-car*. Obviously $\hat\theta \sim N(\hat p_1-\hat p_2, \frac{\hat p_1(1-\hat p_1)}{n_1}+\frac{\hat p_2(1-\hat p_2)}{n_2})$

then as what we have done in section 3.6, find $c_1,c_2$ s.t. $P(c_1<\theta<c_2)=1-\alpha$
similarly, we have 
$$\frac{\hat\theta-\theta}{\hat\sigma(\theta) }\sim N(0,1)$$
so apply all the conclusions we have proved above, we can see that the 95% CIs are (we actually have three proportions, so we can do it for three times):
$$CI(\hat p_1-\hat p_2)=((\hat p_1-\hat p_2)+Z_{\alpha/2}\sqrt{\frac{\hat p_1(1-\hat p_1)}{n_1}+\frac{\hat p_2(1-\hat p_2)}{n_2}},(\hat p_1-\hat p_2)-Z_{\alpha/2}\sqrt{\frac{\hat p_1(1-\hat p_1)}{n_1}+\frac{\hat p_2(1-\hat p_2)}{n_2}}$$

```{r echo=FALSE}
s=sqrt(table(data_bigcar$Drive)/n2*(1-table(data_bigcar$Drive)/n2)/n2+table(data_smallcar$Drive)/n1*(1-table(data_smallcar$Drive)/n1)/n1)
s
print("the lower bounds of the 95% CIs are:")
table(data_bigcar$Drive)/n2-table(data_smallcar$Drive)/n1+qnorm(0.025)*s
print("the uuper bounds of the 95% CIs are:")
table(data_bigcar$Drive)/n2-table(data_smallcar$Drive)/n1-qnorm(0.025)*s
```

## 4.6 Compare the proportion of Section 4.4 for the two subgrups by means of hypothesis testing of equality of proportions.
we can adapt the definitions of general case in section 4.5 and determine the hypothesis we are using. We also assume $\theta=p_1-p_2$, if $p_1=p_2$, obviously $\theta=0$, so our null hypothesis is $H_0: \theta=0$, and the alternative hypothesis is:$H_1: \theta \ne 0$. 
$$\hat\theta \sim N(\hat p_1-\hat p_2, \frac{\hat p_1(1-\hat p_1)}{n_1}+\frac{\hat p_2(1-\hat p_2)}{n_2})$$
$$Z=\frac{\hat\theta-\theta}{\hat\sigma(\theta) }\sim N(0,1)$$
since we are checking if $\theta=0$,
$$Z=\frac{\hat\theta}{\hat\sigma(\theta) }\sim N(0,1)$$
And we will reject the null hypothesis if $Z_{obs}<Z_{0.025} orZ{obs}>Z_{0.975}$(means the proportions in two groups are same), otherwise we will accept the null hypothesis(means they are not same). ($\alpha=0.05$ and $Z_{0.025}=-1.96,Z_{0.975}=1.96$)

```{r echo=FALSE}
(table(data_bigcar$Drive)/n2-table(data_smallcar$Drive)/n1)/s
```
We can see that we should reject that the proportions are same, for all three types.


# Additional analyze: using bootstrap to interpret the estimator

In section 3, we interpret the properties of two estimators *sample mean*,*sample median value* of the population mean. Considering our sample size is large enough, we can use *CLT* to obtain the approximated distribution of *sample mean*, thus is not very hard to obtain it's properties including *unbiasedness*,*efficiency* even more.

However, as shown in section 3, is always very hard to get the exact distribution of *sample median value* ,especially without normal distribution. For this reason, we only gave the expectation and variance of the *sample median value* from some references. 

To do inference for estimator or the parameters with a more complex distribution, we can now introduce *bootstrap*. The *bootstrap* method mainly using the idea of *resampling*. Here is a brief introduction of this method.

Assuming we have a $s.r.s. \space (x_1,x_2,\dots,x_1000)$ from a unknown distribution $X\sim F(.)$, similarly, we want to estimate the population mean of X by sample mean.
we first randomly choose 500 items form the whole sample, and compute the mean value and record it as *m_1*, then we repeat this step for 1000 times. Here we are having a set of sample means$(m_1,m_2,\dots,m_{500})$, and with this new set, we can give the expectation and variance of our estimator,
$$exp(samplemean)= \overline m, var(sample mean)=var(m)$$
further more, we can reorganize the set $(m_1,m_2,\dots,m_{500})$ from the least to the most, and so we can determine the upper-bound and the lower-bound with 95% quantiles (actually 2.5% and 97.5%) of the 95% confidence interval. that is:

$$CI(sample mean)=(m_{(0.025*500)},m_{(0.975*500)})$$
Meanwhile, if we want to estimate the sample median value, all we need to do is to compute the median value of each sub-sample.

In R, we can use the package $boostrap$, or as we are not doing a complex computation ,we can do it manually. 

```{r}
n_iteration=1000 # number of repetition
i_resample=100 # size of each subsample
vector_mean=vector()# the set of mean values
vector_median=vector()# the set of median values

for(i in 1:n_iteration){
  m=sample(EURprice,size=i_resample,replace = FALSE)
  vector_mean[i]=mean(m)
  vector_median[i]=median(m)
}
mean_exp=mean(vector_mean)
median_exp=mean(vector_median)

mean_var=var(vector_mean)
median_var=var(vector_median)
```

we can visualize these two vectors:

```{r echo=FALSE}
mfrow=c(1,2)
hist(vector_mean)
hist(vector_median)
```

this way we obtained two estimator of population mean by $bootstrap$ methods. And we can give their expectations and variances
```{r echo=FALSE}
paste("the expectation of sample mean is:", mean_exp)
paste("the expectation of sample median is:", median_exp)
paste("the variance of the sample mean is:",mean_var)
paste("the variance of the sample median is:",median_var)
```

To obtain their confidence intervals, we need to reorganize the vectors and get the quantiles

```{r}
mean_ordered=sort(vector_mean)
median_ordered=sort(vector_median)

upper_mean=mean_ordered[975]
lower_mean=mean_ordered[25]
## confidence interval bounds of sample mean
upper_med=median_ordered[975]
lower_med=median_ordered[25]
## confidence interval bounds of sample median
```

thus we have the estimators' 95% confidence intervals:
  ```{r echo=FALSE}
print("the 95% CI of sample mean is: ")
c(lower_mean,upper_mean)

print("the 95% CI of sample median is:")
c(lower_med,upper_med)
```

we can see that there is an obvious difference between the expectations, and the variance of sample means is larger, that is to say, more scattered.

# Conclusions

## Summarize and comment on the most important conclusions of your analyses

## Mention limitations and possible extensions of this project

